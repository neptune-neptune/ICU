{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9648356,"sourceType":"datasetVersion","datasetId":5892759}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def get_files_of_type(parent_path:str, filetype:str, as_dict:bool=False) :\n\n    assert os.path.isdir(parent_path), f'{parent_path} is not a valid directory.'\n\n    path_list = sorted([os.path.join(root,file)\n                        for root,_,files in os.walk(parent_path)\n                        for file in files if file.endswith(filetype)])\n\n    assert len(path_list) > 0, f'{parent_path} contains 0 files of file type {filetype}.'\n    \n    if as_dict :\n        \n        path_dict = {\".\".join(os.path.split(path)[-1].split(\".\")[:-1]):path for path in path_list}\n\n        return path_dict\n\n    return path_list\n    \n# Helper function convert any column with header with substring \"time\" to pandas.datetime\n# Taken from Young Sang Choi\ndef dataframe_datetime(df) :\n    \n    for c in [c for c in df.columns if 'time' in c] :\n\n        try :\n            df[c] = pd.to_datetime(df[c])\n        except Exception :\n            pass\n\n    return df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-18T02:08:10.452527Z","iopub.execute_input":"2024-10-18T02:08:10.452911Z","iopub.status.idle":"2024-10-18T02:08:10.459209Z","shell.execute_reply.started":"2024-10-18T02:08:10.452880Z","shell.execute_reply":"2024-10-18T02:08:10.458558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport gc\n\n# Import icu stays records \nicustays_df = pd.read_csv('/kaggle/input/data-icu/icustays.csv',low_memory=True)\n# Import hospital admissions records\nadmissions_df = pd.read_csv('/kaggle/input/data-icu/admissions.csv',low_memory=True)\n# Use atetime conversion\nicustays_df = dataframe_datetime(icustays_df)\nadmissions_df = dataframe_datetime(admissions_df)\n\n# Check that all hadm_ids for icu stays table are in the admissions table\nassert set(icustays_df['hadm_id']).issubset(set(admissions_df['hadm_id']))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:15:54.359150Z","iopub.execute_input":"2024-10-18T02:15:54.359501Z","iopub.status.idle":"2024-10-18T02:15:57.276650Z","shell.execute_reply.started":"2024-10-18T02:15:54.359470Z","shell.execute_reply":"2024-10-18T02:15:57.275841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"icustays_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:16:24.392531Z","iopub.execute_input":"2024-10-18T02:16:24.393022Z","iopub.status.idle":"2024-10-18T02:16:24.409590Z","shell.execute_reply.started":"2024-10-18T02:16:24.392990Z","shell.execute_reply":"2024-10-18T02:16:24.408967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"admissions_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:16:45.524999Z","iopub.execute_input":"2024-10-18T02:16:45.525330Z","iopub.status.idle":"2024-10-18T02:16:45.539040Z","shell.execute_reply.started":"2024-10-18T02:16:45.525303Z","shell.execute_reply":"2024-10-18T02:16:45.538295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \n# Keep icustays with lengths of stay greater or equal to 1 day (taken from admissions_df)\nicustays_df = icustays_df[icustays_df['los'] >= 1]\nicu_hadm_ids = set(icustays_df['hadm_id']) & set(admissions_df['hadm_id'])\nadmissions_df = admissions_df[admissions_df['hadm_id'].isin(icu_hadm_ids)]\nsubjects = set(admissions_df['subject_id'])\nstays = set(icustays_df['stay_id'])\n#print(f'{icustays_df.shape=}')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:19:20.093005Z","iopub.execute_input":"2024-10-18T02:19:20.093359Z","iopub.status.idle":"2024-10-18T02:19:20.165981Z","shell.execute_reply.started":"2024-10-18T02:19:20.093328Z","shell.execute_reply":"2024-10-18T02:19:20.165154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain patient age (based on HIPAA regulations) and gender - gender, 1=female, 0=male\n# Adapted from Young Sang Choi\npatients_df = pd.read_csv('/kaggle/input/data-icu/patients.csv')\npatients_df = patients_df[patients_df['subject_id'].isin(subjects)]\npatients_df['gender'] = np.array(patients_df['gender'] == 'F').astype(int)\nanchor_age_tuples = patients_df.apply(lambda row : (row['subject_id'], row['anchor_age'], row['anchor_year']), 1)\nanchor_age_dict = {subject_id:{'anchor_age':anchor_age, 'anchor_year':anchor_year} for subject_id,anchor_age,anchor_year in anchor_age_tuples}\ngender_dict = dict(zip(patients_df['subject_id'], patients_df['gender']))\nicustays_df['age'] = icustays_df.apply(lambda row : anchor_age_dict[row['subject_id']]['anchor_age'] + (row['intime'].year - anchor_age_dict[row['subject_id']]['anchor_year']), 1)\nicustays_df['age'] = icustays_df.apply(lambda row : row['age'] if row['age'] <= 90 else 90, 1) # HIPAA deidentification rule\nicustays_df['gender'] = icustays_df.apply(lambda row: gender_dict[row['subject_id']], 1)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:19:21.119187Z","iopub.execute_input":"2024-10-18T02:19:21.119507Z","iopub.status.idle":"2024-10-18T02:19:23.761261Z","shell.execute_reply.started":"2024-10-18T02:19:21.119481Z","shell.execute_reply":"2024-10-18T02:19:23.760547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add insurance and admission location to each subject_id in icustays_df\n# Adapted from Young Sang Choi\nadmissions_df = admissions_df[admissions_df['subject_id'].isin(subjects)]\ninsurance_dict = dict(zip(admissions_df['subject_id'], admissions_df['insurance']))\nicustays_df['insurance'] = icustays_df['subject_id'].map(insurance_dict)\nadmloc_dict = dict(zip(admissions_df['subject_id'], admissions_df['admission_location']))\nicustays_df['admission location'] = icustays_df['subject_id'].map(admloc_dict)\nrace_dict = dict(zip(admissions_df['subject_id'], admissions_df['race']))\nicustays_df['race'] = icustays_df['subject_id'].map(race_dict)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:22:35.541776Z","iopub.execute_input":"2024-10-18T02:22:35.542743Z","iopub.status.idle":"2024-10-18T02:22:35.645558Z","shell.execute_reply.started":"2024-10-18T02:22:35.542697Z","shell.execute_reply":"2024-10-18T02:22:35.644762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delete tuples not needed anymore\ndel anchor_age_tuples\ndel anchor_age_dict\ndel gender_dict","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:22:41.330705Z","iopub.execute_input":"2024-10-18T02:22:41.331097Z","iopub.status.idle":"2024-10-18T02:22:41.341343Z","shell.execute_reply.started":"2024-10-18T02:22:41.331065Z","shell.execute_reply":"2024-10-18T02:22:41.340546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# Import vitals table (chartevents)\n# Adapted from Young Sang Choi\nchartevent_definitions = pd.read_csv('/kaggle/input/data-icu/d_items.csv',low_memory=True)\nchartevent_definitions = chartevent_definitions[(chartevent_definitions['linksto'] == 'chartevents') & (chartevent_definitions['category'] == 'Routine Vital Signs')]\nroutine_vital_items = chartevent_definitions['itemid'].values\nprint(f'Number of unique routine vital sign item ids: {len(routine_vital_items)}')\n\nchartevents = pd.read_csv('/kaggle/input/data-icu/chartevents.csv', chunksize = 10000000)\nicu_stay_ids = set(icustays_df['stay_id'])\nsubjects = set(icustays_df['subject_id'])\n\n\nfor p in [os.path.join('.', 'data'), os.path.join('.', 'data', 'mimic_chartevents')] :\n    if not os.path.isdir(p) :\n        os.mkdir(p)\n\nmimic_chartevents_parent = os.path.join('.', 'data', 'mimic_chartevents')\n\nif len(os.listdir(mimic_chartevents_parent)) == 0 : \n\n    for i,chunk in enumerate(chartevents) : \n        \n        original_size = chunk.shape\n        chunk = chunk[(chunk['stay_id'].isin(icu_stay_ids)) & (chunk['itemid'].isin(routine_vital_items))]\n        chunk.to_csv(os.path.join(mimic_chartevents_parent, f'mimic_vitals_{i}.csv'), index = False)\n        print(f'Chunk {i}: selected {chunk.shape[0]} from {original_size[0]} rows.')\n\nchartevents_df = pd.concat([pd.read_csv(path) for path in get_files_of_type(mimic_chartevents_parent, filetype = 'csv')], axis=0, ignore_index=True)\nchartevents_df.reset_index(drop = True, inplace = True)\nchartevents_df['charttime'] = pd.to_datetime(chartevents_df['charttime'])\n\n#print(f'{chartevents_df.shape=}')\n\n# Group the chartevents_df by 'stay_id' and 'itemid', and keep the first row for each group - first vital signs for each patient\nfiltered_chartevents_df = chartevents_df.groupby(['stay_id', 'itemid']).first().reset_index()\n#print(f'{filtered_chartevents_df.shape=}')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:23:42.193913Z","iopub.execute_input":"2024-10-18T02:23:42.194289Z","iopub.status.idle":"2024-10-18T02:39:15.016674Z","shell.execute_reply.started":"2024-10-18T02:23:42.194261Z","shell.execute_reply":"2024-10-18T02:39:15.015625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the chartevents and icustay DataFrames based on the condition\nfiltered_chartevents_df = pd.merge(\n    filtered_chartevents_df,\n    icustays_df,\n    how='inner',\n    on=['stay_id','subject_id','hadm_id']\n)\n#filtered_chartevents_df.rename(columns={'stay_id_x': 'stay_id'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:43:09.960087Z","iopub.execute_input":"2024-10-18T02:43:09.961067Z","iopub.status.idle":"2024-10-18T02:43:10.229823Z","shell.execute_reply.started":"2024-10-18T02:43:09.961024Z","shell.execute_reply":"2024-10-18T02:43:10.228597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n# Obtain labevents table for all patients in cohort\nlabs_df = pd.read_csv('/kaggle/input/data-icu/labevents.csv',low_memory=True)\nlabs_df = labs_df[labs_df['subject_id'].isin(subjects)]\nlabs_df = dataframe_datetime(labs_df)\nprint(f'{labs_df.shape=}')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:43:28.908339Z","iopub.execute_input":"2024-10-18T02:43:28.908781Z","iopub.status.idle":"2024-10-18T02:49:29.152132Z","shell.execute_reply.started":"2024-10-18T02:43:28.908749Z","shell.execute_reply":"2024-10-18T02:49:29.151200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the DataFrames based on the condition\nfiltered_labs_df = pd.merge(\n    labs_df,\n    icustays_df,\n    how='inner',\n    left_on=['subject_id', 'hadm_id'],\n    right_on=['subject_id', 'hadm_id']\n)\n\nfiltered_labs_df['charttime'] = pd.to_datetime(filtered_labs_df['charttime'])\n\n# Filter rows where 'charttime' is between 'intime' and 'outtime'\ncondition = (filtered_labs_df['charttime'] >= filtered_labs_df['intime']) & (filtered_labs_df['charttime'] <= filtered_labs_df['outtime'])\nfiltered_labs_df.loc[condition, 'stay_id'] = filtered_labs_df.loc[condition, 'stay_id']\n\n# Display the resulting DataFrame\n#print(filtered_labs_df[['subject_id','hadm_id','stay_id','itemid','valuenum']])\n#print(f'{filtered_labs_df.shape=}')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:51:02.440687Z","iopub.execute_input":"2024-10-18T02:51:02.441074Z","iopub.status.idle":"2024-10-18T02:51:24.877677Z","shell.execute_reply.started":"2024-10-18T02:51:02.441042Z","shell.execute_reply":"2024-10-18T02:51:24.876788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean and convert the 'value' column to numeric\nfiltered_labs_df['value'] = pd.to_numeric(filtered_labs_df['value'], errors='coerce')\n\n# Check for duplicate itemid within each stay_id group\nduplicates_for_stay_id = filtered_labs_df[filtered_labs_df.duplicated(subset=['stay_id', 'itemid'], keep=False)]\n\n# Calculate the average value for duplicate itemid within each stay_id group\naverages_for_duplicates = duplicates_for_stay_id.groupby(['stay_id', 'itemid'])['value'].mean().reset_index()\n\n# Merge the calculated averages back into the original DataFrame\nfiltered_labs_df = pd.merge(filtered_labs_df, averages_for_duplicates, how='left', on=['stay_id', 'itemid'], suffixes=('', '_avg'))\n\n# Fill NaN values in the original 'value' column with the calculated averages\nfiltered_labs_df['value'].fillna(filtered_labs_df['value_avg'], inplace=True)\n\n# Drop the redundant 'value_avg' column\nfiltered_labs_df.drop(columns=['value_avg'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:51:29.636240Z","iopub.execute_input":"2024-10-18T02:51:29.636572Z","iopub.status.idle":"2024-10-18T02:52:16.466103Z","shell.execute_reply.started":"2024-10-18T02:51:29.636543Z","shell.execute_reply":"2024-10-18T02:52:16.465293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\npharm_df = pd.read_csv('/kaggle/input/data-icu/prescriptions.csv',low_memory=True)\n# Merge the DataFrames based on the condition\nfiltered_pharm_df = pd.merge(\n    pharm_df,\n    icustays_df,\n    how='inner',\n    left_on=['subject_id', 'hadm_id'],\n    right_on=['subject_id', 'hadm_id']\n)\n\nfiltered_pharm_df['starttime'] = pd.to_datetime(filtered_pharm_df['starttime'])\n\n# Filter rows where 'charttime' is between 'intime' and 'outtime'\ncondition = (filtered_pharm_df['starttime'] >= filtered_pharm_df['intime']) & (filtered_pharm_df['starttime'] <= filtered_pharm_df['outtime'])\nfiltered_pharm_df.loc[condition, 'stay_id'] = filtered_pharm_df.loc[condition, 'stay_id']\n\n# Display the resulting DataFrame\n#print(filtered_pharm_df[['subject_id','hadm_id','stay_id','drug']])\n#print(f'{filtered_pharm_df.shape=}')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:52:33.959983Z","iopub.execute_input":"2024-10-18T02:52:33.960353Z","iopub.status.idle":"2024-10-18T02:53:52.293402Z","shell.execute_reply.started":"2024-10-18T02:52:33.960321Z","shell.execute_reply":"2024-10-18T02:53:52.292599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dataframes for analysis\nanalysis_chartevents_df = filtered_chartevents_df[['stay_id', 'subject_id', 'itemid', 'valuenum','insurance','gender','age','race']]\nanalysis_labs_df = filtered_labs_df[['subject_id', 'stay_id', 'itemid', 'value','insurance','gender','age','race']]\nanalysis_pharm_df = filtered_pharm_df[['subject_id', 'stay_id', 'drug','insurance','gender','age','race']]","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:55:00.989576Z","iopub.execute_input":"2024-10-18T02:55:00.990391Z","iopub.status.idle":"2024-10-18T02:55:02.233524Z","shell.execute_reply.started":"2024-10-18T02:55:00.990352Z","shell.execute_reply":"2024-10-18T02:55:02.232741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only keep columns needed for each dataframe \n_chartevents_df = filtered_chartevents_df[['stay_id', 'subject_id', 'itemid', 'valuenum']]\n_labs_df = filtered_labs_df[['subject_id', 'stay_id', 'itemid', 'value']]\n_pharm_df = filtered_pharm_df[['subject_id', 'stay_id', 'drug']]","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:55:08.570236Z","iopub.execute_input":"2024-10-18T02:55:08.571078Z","iopub.status.idle":"2024-10-18T02:55:08.919110Z","shell.execute_reply.started":"2024-10-18T02:55:08.571041Z","shell.execute_reply":"2024-10-18T02:55:08.918322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace item_id values with names to be more legible \n\nd_items = pd.read_csv('/kaggle/input/data-icu/d_items.csv',low_memory=True)\n# Create a dictionary mapping itemid to label from chartevent_definitions\nitemid_to_label = d_items.set_index('itemid')['label'].to_dict()\n# In-place replace the itemid column with label\n_chartevents_df['itemid'] = _chartevents_df['itemid'].map(itemid_to_label)\n\n# Create a dictionary mapping itemid to label from d_labitems\nd_labitems = pd.read_csv('/kaggle/input/data-icu/d_labitems.csv',low_memory=True)\nitemid_to_label = d_labitems.set_index('itemid')['label'].to_dict()\n# In-place replace the itemid column with label\n_labs_df['itemid'] = _labs_df['itemid'].map(itemid_to_label)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:55:31.123595Z","iopub.execute_input":"2024-10-18T02:55:31.124321Z","iopub.status.idle":"2024-10-18T02:55:32.027288Z","shell.execute_reply.started":"2024-10-18T02:55:31.124284Z","shell.execute_reply":"2024-10-18T02:55:32.026551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pivot the DataFrame without itemid and fill NaN with 0\nfinal_chartevents = _chartevents_df.pivot_table(\n    index=['subject_id', 'stay_id'],\n    columns='itemid',\n    values='valuenum',\n    fill_value=0  # Replace NaN with 0\n)\n\n# Reset the index if needed\nfinal_chartevents = final_chartevents.reset_index()\n\n# Display the resulting DataFrame\n#final_chartevents.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:56:29.759059Z","iopub.execute_input":"2024-10-18T02:56:29.759445Z","iopub.status.idle":"2024-10-18T02:56:30.080555Z","shell.execute_reply.started":"2024-10-18T02:56:29.759416Z","shell.execute_reply":"2024-10-18T02:56:30.079567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert 'value' column to numeric, coerce non-numeric values to NaN\n_labs_df['value'] = pd.to_numeric(_labs_df['value'], errors='coerce')\n\n# Pivot the DataFrame without itemid and fill NaN with 0\nfinal_labs = _labs_df.pivot_table(\n    index=['subject_id', 'stay_id'],\n    columns='itemid',\n    values='value',\n    fill_value=0  # Replace NaN with 0\n)\n\n# Reset the index if needed\nfinal_labs = final_labs.reset_index()\n\n# Display the resulting DataFrame\n#final_labs.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:56:36.622654Z","iopub.execute_input":"2024-10-18T02:56:36.622993Z","iopub.status.idle":"2024-10-18T02:56:43.094320Z","shell.execute_reply.started":"2024-10-18T02:56:36.622964Z","shell.execute_reply":"2024-10-18T02:56:43.093506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new column 'value' and set it to 1 for all rows\n_pharm_df['value'] = 1\n\n# Pivot the DataFrame\nfinal_pharm = _pharm_df.pivot_table(index=['subject_id', 'stay_id'], columns='drug', values='value', fill_value=0)\n\n# Reset the index if needed\nfinal_pharm = final_pharm.reset_index()\n#final_pharm.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:56:43.583866Z","iopub.execute_input":"2024-10-18T02:56:43.584588Z","iopub.status.idle":"2024-10-18T02:56:50.986091Z","shell.execute_reply.started":"2024-10-18T02:56:43.584552Z","shell.execute_reply":"2024-10-18T02:56:50.985289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the first two pivoted DataFrames\nicu_data = pd.merge(icustays_df, final_chartevents, on=['subject_id', 'stay_id'], how='inner')\n\n# Merge the third pivoted DataFrame\nicu_data = pd.merge(icu_data, final_labs, on=['subject_id', 'stay_id'], how='inner')\n\n# Merge the icustays_df DataFrame\nicu_data = pd.merge(icu_data, final_pharm, on=['subject_id', 'stay_id'], how='inner')\n\n# Display the resulting DataFrame\nicu_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:56:50.987387Z","iopub.execute_input":"2024-10-18T02:56:50.987644Z","iopub.status.idle":"2024-10-18T02:56:53.574867Z","shell.execute_reply.started":"2024-10-18T02:56:50.987619Z","shell.execute_reply":"2024-10-18T02:56:53.574185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(icu_data['stay_id'].nunique())","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:57:02.744089Z","iopub.execute_input":"2024-10-18T02:57:02.744422Z","iopub.status.idle":"2024-10-18T02:57:02.751148Z","shell.execute_reply.started":"2024-10-18T02:57:02.744394Z","shell.execute_reply":"2024-10-18T02:57:02.750356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mapping dictionary for race renaming\nrace_mapping = {\n    'ASIAN - ASIAN INDIAN': 'ASIAN',\n    'ASIAN - CHINESE': 'ASIAN',\n    'ASIAN - KOREAN': 'ASIAN',\n    'ASIAN - SOUTH EAST ASIAN': 'ASIAN',\n    'UNKNOWN': 'OTHER',\n    'UNABLE TO OBTAIN': 'OTHER',\n    'PATIENT DECLINED TO ANSWER': 'OTHER',\n    'WHITE - RUSSIAN': 'WHITE',\n    'PORTUGUESE': 'WHITE',\n    'WHITE - BRAZILIAN': 'WHITE',\n    'WHITE - OTHER EUROPEAN': 'WHITE',\n    'WHITE - EASTERN EUROPEAN': 'WHITE', \n    'BLACK/CAPE VERDEAN':'BLACK/AFRICAN AMERICAN',\n    'BLACK/AFRICAN':'BLACK/AFRICAN AMERICAN',\n    'BLACK/CARIBBEAN ISLAND':'BLACK/AFRICAN AMERICAN',\n    'HISPANIC/LATINO - CUBAN': 'HISPANIC/LATINO',\n    'HISPANIC/LATINO - PUERTO RICAN': 'HISPANIC/LATINO',\n    'HISPANIC OR LATINO': 'HISPANIC/LATINO',\n    'HISPANIC/LATINO - SALVADORAN': 'HISPANIC/LATINO',\n    'HISPANIC/LATINO - DOMINICAN': 'HISPANIC/LATINO',\n    'HISPANIC/LATINO - HONDURAN': 'HISPANIC/LATINO',\n    'HISPANIC/LATINO - COLUMBIAN': 'HISPANIC/LATINO',\n    'HISPANIC/LATINO - GUATEMALAN': 'HISPANIC/LATINO',\n    'HISPANIC/LATINO - CENTRAL AMERICAN': 'HISPANIC/LATINO',\n    'HISPANIC/LATINO - MEXICAN': 'HISPANIC/LATINO',\n    'SOUTH AMERICAN': 'HISPANIC/LATINO',\n    'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER': 'PACIFIC ISLANDER',\n    'AMERICAN INDIAN/ALASKA NATIVE': 'PACIFIC ISLANDER',\n    'NATIVE HAWAIIAN/PACIFIC ISLANDER': 'PACIFIC ISLANDER'\n    \n}\n# Replace values in the 'race' column using the mapping dictionary for all dataframes \nicu_data['race'] = icu_data['race'].replace(race_mapping)\nanalysis_chartevents_df['race'] = analysis_chartevents_df['race'].replace(race_mapping)\nanalysis_labs_df['race'] = analysis_labs_df['race'].replace(race_mapping)\nanalysis_pharm_df['race'] = analysis_pharm_df['race'].replace(race_mapping)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:58:04.571458Z","iopub.execute_input":"2024-10-18T02:58:04.572160Z","iopub.status.idle":"2024-10-18T02:58:57.739890Z","shell.execute_reply.started":"2024-10-18T02:58:04.572125Z","shell.execute_reply":"2024-10-18T02:58:57.739036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape of the cleaned data\nprint(icu_data.shape[0])\nprint(icu_data.shape[1])","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:58:57.741346Z","iopub.execute_input":"2024-10-18T02:58:57.741607Z","iopub.status.idle":"2024-10-18T02:58:57.745431Z","shell.execute_reply.started":"2024-10-18T02:58:57.741582Z","shell.execute_reply":"2024-10-18T02:58:57.744823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Export csv file with all the cleaned data\nicu_data.to_csv('icu_data_Nolm.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T02:59:11.230229Z","iopub.execute_input":"2024-10-18T02:59:11.231139Z","iopub.status.idle":"2024-10-18T03:01:58.977081Z","shell.execute_reply.started":"2024-10-18T02:59:11.231101Z","shell.execute_reply":"2024-10-18T03:01:58.976203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Stadistics","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nicu_data = pd.read_csv('/kaggle/working/icu_data_Nolm.csv',low_memory=True)\nicu_stays = pd.read_csv('/kaggle/input/data-icu/icustays.csv', low_memory=True)\nadmissions = pd.read_csv('/kaggle/input/data-icu/admissions.csv',low_memory=True)\npatients = pd.read_csv('/kaggle/input/data-icu/patients.csv', low_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:03:54.225976Z","iopub.execute_input":"2024-10-18T03:03:54.226364Z","iopub.status.idle":"2024-10-18T03:04:34.869082Z","shell.execute_reply.started":"2024-10-18T03:03:54.226334Z","shell.execute_reply":"2024-10-18T03:04:34.868290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def percentage(df, column, condition):\n\n    condition_number = len(df.loc[df[column]==condition])\n    result = condition_number / len(df)\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:04:37.225489Z","iopub.execute_input":"2024-10-18T03:04:37.226029Z","iopub.status.idle":"2024-10-18T03:04:37.230420Z","shell.execute_reply.started":"2024-10-18T03:04:37.225976Z","shell.execute_reply":"2024-10-18T03:04:37.229684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"icu_stays.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:04:39.215915Z","iopub.execute_input":"2024-10-18T03:04:39.216738Z","iopub.status.idle":"2024-10-18T03:04:39.221689Z","shell.execute_reply.started":"2024-10-18T03:04:39.216702Z","shell.execute_reply":"2024-10-18T03:04:39.220882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"admissions['insurance'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:04:40.235934Z","iopub.execute_input":"2024-10-18T03:04:40.236301Z","iopub.status.idle":"2024-10-18T03:04:40.260699Z","shell.execute_reply.started":"2024-10-18T03:04:40.236268Z","shell.execute_reply":"2024-10-18T03:04:40.260033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percentage(admissions, 'insurance', 'Other')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:04:40.955842Z","iopub.execute_input":"2024-10-18T03:04:40.956586Z","iopub.status.idle":"2024-10-18T03:04:41.031777Z","shell.execute_reply.started":"2024-10-18T03:04:40.956550Z","shell.execute_reply":"2024-10-18T03:04:41.031147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nicu_data['hadm_id'].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:04:50.954477Z","iopub.execute_input":"2024-10-18T03:04:50.954867Z","iopub.status.idle":"2024-10-18T03:04:50.963348Z","shell.execute_reply.started":"2024-10-18T03:04:50.954835Z","shell.execute_reply":"2024-10-18T03:04:50.962588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"icu_stays['hadm_id'].loc[icu_stays['los']>1].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:04:53.027648Z","iopub.execute_input":"2024-10-18T03:04:53.028027Z","iopub.status.idle":"2024-10-18T03:04:53.036666Z","shell.execute_reply.started":"2024-10-18T03:04:53.027995Z","shell.execute_reply":"2024-10-18T03:04:53.035918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(icu_data[icu_data['age'] >= 18])","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:04:59.635643Z","iopub.execute_input":"2024-10-18T03:04:59.636032Z","iopub.status.idle":"2024-10-18T03:05:00.447117Z","shell.execute_reply.started":"2024-10-18T03:04:59.635999Z","shell.execute_reply":"2024-10-18T03:05:00.446326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analysis_labs_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:05:04.654811Z","iopub.execute_input":"2024-10-18T03:05:04.655455Z","iopub.status.idle":"2024-10-18T03:05:04.665003Z","shell.execute_reply.started":"2024-10-18T03:05:04.655423Z","shell.execute_reply":"2024-10-18T03:05:04.664310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualizations for Disparities in Data","metadata":{}},{"cell_type":"markdown","source":"## Fig. 1: Depiction of Race breakdown","metadata":{}},{"cell_type":"markdown","source":"### (a) Average number of medications per ICU stay","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming you have a DataFrame called analysis_chartevents_df\n\n# Group by 'stay_id' and 'insurance_grouping' and calculate the average number of chart events\naverage_pharm = analysis_pharm_df.groupby(['stay_id', 'race']).size().reset_index(name='pharm_count').groupby('race')['pharm_count'].mean()\n\n# Plotting the bar chart\nax = average_pharm.plot(kind='bar', color='skyblue', edgecolor='black', figsize=(10,5))\n\n# Add labels to the bars\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\nplt.title('Average Number of Medications per first 24hr ICU Stay by Race')\nplt.xlabel('Race')\nplt.ylabel('Average Number of Medications Taken')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:08:20.339502Z","iopub.execute_input":"2024-10-18T03:08:20.339977Z","iopub.status.idle":"2024-10-18T03:08:21.070799Z","shell.execute_reply.started":"2024-10-18T03:08:20.339944Z","shell.execute_reply":"2024-10-18T03:08:21.070114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (b) average number of labs per ICU stay","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming you have a DataFrame called analysis_chartevents_df\n\n# Group by 'stay_id' and 'insurance_grouping' and calculate the average number of chart events\naverage_labs = analysis_labs_df.groupby(['stay_id', 'race']).size().reset_index(name='labs_count').groupby('race')['labs_count'].mean()\n\n# Plotting the bar chart\nax = average_labs.plot(kind='bar', color='skyblue', edgecolor='black', figsize=(10,5))\n\n# Add labels to the bars\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\nplt.title('Average Number of Labs per first 24hr ICU Stay by Race')\nplt.xlabel('Race')\nplt.ylabel('Average Number of Labs Taken')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:09:23.793911Z","iopub.execute_input":"2024-10-18T03:09:23.794744Z","iopub.status.idle":"2024-10-18T03:09:26.333455Z","shell.execute_reply.started":"2024-10-18T03:09:23.794710Z","shell.execute_reply":"2024-10-18T03:09:26.332695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (c) average number of vitals per ICU stay","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Group by 'stay_id' and 'insurance_grouping' and calculate the average number of chart events\naverage_chartevents = analysis_chartevents_df.groupby(['stay_id', 'race']).size().reset_index(name='chartevents_count').groupby('race')['chartevents_count'].mean()\n\n# Plotting the bar chart\nax = average_chartevents.plot(kind='bar', color='skyblue', edgecolor='black', figsize=(10,5))\n\n# Add labels to the bars\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\nplt.title('Average Number of Vitals Taken per first 24hr ICU Stay by Race')\nplt.xlabel('Race')\nplt.ylabel('Average Number of Vitals Taken')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:09:10.693320Z","iopub.execute_input":"2024-10-18T03:09:10.694172Z","iopub.status.idle":"2024-10-18T03:09:10.947582Z","shell.execute_reply.started":"2024-10-18T03:09:10.694135Z","shell.execute_reply":"2024-10-18T03:09:10.946813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fig. 2: Depiction of gender breakdown","metadata":{}},{"cell_type":"markdown","source":"### (a) The average number of medications per ICU stay","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming you have a DataFrame called analysis_chartevents_df\n\n# Group by 'stay_id' and 'insurance_grouping' and calculate the average number of chart events\naverage_pharm = analysis_pharm_df.groupby(['stay_id', 'gender']).size().reset_index(name='pharm_count').groupby('gender')['pharm_count'].mean()\n\n# Plotting the bar chart\nax = average_pharm.plot(kind='bar', color='skyblue', edgecolor='black', figsize=(10,5))\n\n# Add labels to the bars\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\nplt.title('Average Number of Medications Taken per first 24hr ICU Stay by Gender')\nplt.xlabel('Gender')\nplt.ylabel('Average Number of Medications Taken')\nax.set_xticklabels(['Male', 'Female'], rotation=90, ha='right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:10:38.314597Z","iopub.execute_input":"2024-10-18T03:10:38.315367Z","iopub.status.idle":"2024-10-18T03:10:38.724562Z","shell.execute_reply.started":"2024-10-18T03:10:38.315331Z","shell.execute_reply":"2024-10-18T03:10:38.723701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b) the average number of labs per ICU stay","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Group by 'stay_id' and 'insurance_grouping' and calculate the average number of labs\naverage_labs = analysis_labs_df.groupby(['stay_id', 'gender']).size().reset_index(name='labs_count').groupby('gender')['labs_count'].mean()\n\n# Plotting the bar chart\nax = average_labs.plot(kind='bar', color='skyblue', edgecolor='black', figsize=(10, 5))\n\n# Add labels to the bars\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\nplt.title('Average Number of Labs per first 24hr ICU Stay by Gender')\nplt.xlabel('Gender')\nplt.ylabel('Average Number of Labs Taken')\n# Combine rotation and custom labels\nax.set_xticklabels(['Male', 'Female'], rotation=90, ha='right')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:11:14.045962Z","iopub.execute_input":"2024-10-18T03:11:14.046340Z","iopub.status.idle":"2024-10-18T03:11:15.343760Z","shell.execute_reply.started":"2024-10-18T03:11:14.046307Z","shell.execute_reply":"2024-10-18T03:11:15.343065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (c) the average number of vitals per ICU stay","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Group by 'stay_id' and 'insurance_grouping' and calculate the average number of chart events\naverage_chartevents = analysis_chartevents_df.groupby(['stay_id', 'gender']).size().reset_index(name='chartevents_count').groupby('gender')['chartevents_count'].mean()\n\n# Plotting the bar chart\nax = average_chartevents.plot(kind='bar', color='skyblue', edgecolor='black', figsize=(10,5))\n\n# Add labels to the bars\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\nplt.title('Average Number of Vitals Taken per first 24hr ICU Stay by Gender')\nplt.xlabel('Gender Grouping')\nplt.ylabel('Average Number of Vitals Taken')\nax.set_xticklabels(['Male', 'Female'], rotation=90, ha='right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:11:00.683106Z","iopub.execute_input":"2024-10-18T03:11:00.683483Z","iopub.status.idle":"2024-10-18T03:11:00.859109Z","shell.execute_reply.started":"2024-10-18T03:11:00.683452Z","shell.execute_reply":"2024-10-18T03:11:00.858457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fig. 3: Depiction of insurance breakdown","metadata":{}},{"cell_type":"markdown","source":"### (a) Average number of medications per ICU stay","metadata":{}},{"cell_type":"code","source":"# Assuming you have a DataFrame called analysis_chartevents_df\n\n# Group by 'stay_id' and 'insurance_grouping' and calculate the average number of chart events\naverage_pharm = analysis_pharm_df.groupby(['stay_id', 'gender']).size().reset_index(name='pharm_count').groupby('gender')['pharm_count'].mean()\n\n# Plotting the bar chart\nax = average_pharm.plot(kind='bar', color='skyblue', edgecolor='black', figsize=(10,5))\n\n# Add labels to the bars\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\nplt.title('Average Number of Medications Taken per first 24hr ICU Stay by Gender')\nplt.xlabel('Gender')\nplt.ylabel('Average Number of Medications Taken')\nax.set_xticklabels(['Male', 'Female'], rotation=90, ha='right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:15:09.034663Z","iopub.execute_input":"2024-10-18T03:15:09.035098Z","iopub.status.idle":"2024-10-18T03:15:09.428496Z","shell.execute_reply.started":"2024-10-18T03:15:09.035056Z","shell.execute_reply":"2024-10-18T03:15:09.427638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (b) average number of labs per ICU stay","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Group by 'stay_id' and 'insurance_grouping' and calculate the average number of chart events\naverage_labs = analysis_labs_df.groupby(['stay_id', 'insurance']).size().reset_index(name='labs_count').groupby('insurance')['labs_count'].mean()\n\n# Plotting the bar chart\nax = average_labs.plot(kind='bar', color='skyblue', edgecolor='black', figsize=(10, 5))\n\n# Add labels to the bars\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\nplt.title('Average Number of Labs per first 24hr ICU Stay by Insurance')\nplt.xlabel('Insurance')\nplt.ylabel('Average Number of Labs Taken')\n# Rotate x-axis labels\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:13:09.387920Z","iopub.execute_input":"2024-10-18T03:13:09.388289Z","iopub.status.idle":"2024-10-18T03:13:11.884902Z","shell.execute_reply.started":"2024-10-18T03:13:09.388260Z","shell.execute_reply":"2024-10-18T03:13:11.884095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### (c) average number of vitals per ICU stay","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Group by 'stay_id' and 'insurance_grouping' and calculate the average number of chart events\naverage_chartevents = analysis_chartevents_df.groupby(['stay_id', 'insurance']).size().reset_index(name='chartevents_count').groupby('insurance')['chartevents_count'].mean()\n\n# Plotting the bar chart\nax = average_chartevents.plot(kind='bar', color='skyblue', edgecolor='black', figsize=(10, 5))\n\n# Add labels to the bars\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\nplt.title('Average Number of Vitals Taken per first 24hr ICU Stay by Insurance')\nplt.xlabel('Insurance')\nplt.ylabel('Average Number of Vitals Taken')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:16:12.182487Z","iopub.execute_input":"2024-10-18T03:16:12.183305Z","iopub.status.idle":"2024-10-18T03:16:12.462699Z","shell.execute_reply.started":"2024-10-18T03:16:12.183265Z","shell.execute_reply":"2024-10-18T03:16:12.462078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fig. 4: Length of ICU Stay by Insurance and Race","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n# Assuming you have a DataFrame called icu_data\n\n# Set seaborn style to whitegrid\nsns.set_style(\"whitegrid\")\n\n# Plotting the grouped bar plot for average LOS\nplt.figure(figsize=(20, 8))\nax = sns.barplot(x='race', y='los', hue='insurance', data=icu_data, palette='colorblind', estimator=np.mean)\n\nplt.title('Average Length of Stay (LOS) by Insurance and Race')\nplt.xlabel('Race')\nplt.ylabel('Average Length of Stay (LOS)')\n\nplt.legend(title='Insurance', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:17:01.029755Z","iopub.execute_input":"2024-10-18T03:17:01.030856Z","iopub.status.idle":"2024-10-18T03:17:02.107559Z","shell.execute_reply.started":"2024-10-18T03:17:01.030815Z","shell.execute_reply":"2024-10-18T03:17:02.106907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have a DataFrame called icu_data\n\n# Set seaborn style to whitegrid\nsns.set_style(\"whitegrid\")\n\n# Plotting the grouped bar plot for average LOS\nplt.figure(figsize=(20, 8))\nax = sns.barplot(x='insurance', y='los', hue='race', data=icu_data, palette='colorblind', estimator=np.mean)\n\nplt.title('Average Length of Stay (LOS) by Insurance and Race')\nplt.xlabel('Race')\nplt.ylabel('Average Length of Stay (LOS)')\n\nplt.legend(title='Race', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:17:07.251762Z","iopub.execute_input":"2024-10-18T03:17:07.252547Z","iopub.status.idle":"2024-10-18T03:17:08.344546Z","shell.execute_reply.started":"2024-10-18T03:17:07.252516Z","shell.execute_reply":"2024-10-18T03:17:08.343774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ANOVA ANALYSIS","metadata":{}},{"cell_type":"code","source":"grouped_data = icu_data.groupby(['gender', 'insurance', 'race'])['los'].mean()\n\n# To calculate median instead of mean, use the following:\n# grouped_data = icustays_df.groupby(['gender', 'insurance', 'race'])['los'].median()\n\n# Displaying the grouped data\nprint(grouped_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:17:45.467258Z","iopub.execute_input":"2024-10-18T03:17:45.468020Z","iopub.status.idle":"2024-10-18T03:17:45.485135Z","shell.execute_reply.started":"2024-10-18T03:17:45.467982Z","shell.execute_reply":"2024-10-18T03:17:45.484399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import f_oneway\n\n# Assuming 'icustays_df' is your DataFrame\n# Performing one-way ANOVA test for 'gender'\nanova_gender = f_oneway(*[group['los'].values for name, group in icu_data.groupby('gender')])\n\n# Performing one-way ANOVA test for 'insurance'\nanova_insurance = f_oneway(*[group['los'].values for name, group in icu_data.groupby('insurance')])\n\n# Performing one-way ANOVA test for 'race'\nanova_race = f_oneway(*[group['los'].values for name, group in icu_data.groupby('race')])\n\n# Displaying the ANOVA results\nprint(\"ANOVA for Gender:\")\nprint(anova_gender)\n\nprint(\"\\nANOVA for Insurance:\")\nprint(anova_insurance)\n\nprint(\"\\nANOVA for Race:\")\nprint(anova_race)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:18:18.401158Z","iopub.execute_input":"2024-10-18T03:18:18.401951Z","iopub.status.idle":"2024-10-18T03:18:21.213892Z","shell.execute_reply.started":"2024-10-18T03:18:18.401917Z","shell.execute_reply":"2024-10-18T03:18:21.213126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model XGBOOST - Replication","metadata":{}},{"cell_type":"code","source":"condition = (icu_data['los'] > 4).map({True: 'Long', False: 'Short'})\n\n# Create a new column 'los_type' based on the condition\nicu_data['los_type'] = condition","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:18:34.824840Z","iopub.execute_input":"2024-10-18T03:18:34.825170Z","iopub.status.idle":"2024-10-18T03:18:34.832221Z","shell.execute_reply.started":"2024-10-18T03:18:34.825144Z","shell.execute_reply":"2024-10-18T03:18:34.831415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into training and validation data\nfrom sklearn.model_selection import train_test_split\n\n# Assuming 'subject_id' is the patient identifier\npatient_ids = icu_data['subject_id'].unique()\n\n# Split patient_ids into training and testing sets\ntrain_patient_ids, validation_patient_ids = train_test_split(patient_ids, test_size=0.2, random_state=42)\n\n# Filter the dataframe based on the split of patient_ids\ntrain_data = icu_data[icu_data['subject_id'].isin(train_patient_ids)]\nvalidation_data = icu_data[icu_data['subject_id'].isin(validation_patient_ids)]\n\n# Check the shapes of the training and testing sets\nprint(\"Training set shape:\", train_data.shape)\nprint(\"Validation set shape:\", validation_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:18:42.263469Z","iopub.execute_input":"2024-10-18T03:18:42.264346Z","iopub.status.idle":"2024-10-18T03:18:43.626725Z","shell.execute_reply.started":"2024-10-18T03:18:42.264309Z","shell.execute_reply":"2024-10-18T03:18:43.625962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_insurance_counts =train_data['insurance'].value_counts()\nprint(\"Train Data Insurance Counts:\")\nprint(train_insurance_counts)\n\nvalidation_insurance_counts =validation_data['insurance'].value_counts()\nprint(\"Validation Data Insurance Counts:\")\nprint(validation_insurance_counts)\n\ntrain_race_counts =train_data['race'].value_counts()\nprint(\"Train Data Race Counts:\")\nprint(train_race_counts)\n\nvalidation_race_counts =validation_data['race'].value_counts()\nprint(\"Validation Data Race Counts:\")\nprint(validation_race_counts)\n\ntrain_duration_counts =train_data['los_type'].value_counts()\nprint(\"Train Data LOS Counts:\")\nprint(train_duration_counts)\n\nvalidation_duration_counts =validation_data['los_type'].value_counts()\nprint(\"Validation Data LOS Counts:\")\nprint(validation_duration_counts)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:19:04.174650Z","iopub.execute_input":"2024-10-18T03:19:04.175686Z","iopub.status.idle":"2024-10-18T03:19:04.198751Z","shell.execute_reply.started":"2024-10-18T03:19:04.175648Z","shell.execute_reply":"2024-10-18T03:19:04.197922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming 'subject_id' is the patient identifier\npatient_ids_train = train_data['subject_id'].unique()\n\n# Split patient_ids_train into training and testing sets\ntrain_train_ids, test_train_ids = train_test_split(patient_ids_train, test_size=0.2, random_state=42)\n\n# Filter the train_data based on the split of patient_ids_train\ntrain_train_data = train_data[train_data['subject_id'].isin(train_train_ids)]\ntest_train_data = train_data[train_data['subject_id'].isin(test_train_ids)]\n\n# Check the shapes of the training and testing sets\nprint(\"Train Training set shape:\", train_train_data.shape)\nprint(\"Train Testing set shape:\", test_train_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:23:19.042100Z","iopub.execute_input":"2024-10-18T03:23:19.042505Z","iopub.status.idle":"2024-10-18T03:23:19.734292Z","shell.execute_reply.started":"2024-10-18T03:23:19.042474Z","shell.execute_reply":"2024-10-18T03:23:19.733534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install xgboost\nimport pandas as pd\nimport xgboost\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Columns to exclude from features\nexclude_columns = ['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'los', 'los_type', \n                   'age', 'gender', 'insurance', 'race', 'PHYSICIAN REFERRAL']\n\n# Define your feature columns (exclude certain columns)\nfeature_columns = [col for col in train_data.columns if col not in exclude_columns]\n\n# Exclude non-numeric columns before scaling\nnumeric_columns = train_train_data[feature_columns].select_dtypes(include=['float64', 'int64']).columns\nX_train = train_train_data[numeric_columns]\ny_train = train_train_data['los_type']\nX_test = test_train_data[numeric_columns]\ny_test = test_train_data['los_type']\n\n# Convert 'Short' and 'Long' to numerical labels (0 and 1)\ny_train = y_train.map({'Short': 0, 'Long': 1})\ny_test = y_test.map({'Short': 0, 'Long': 1})\n\n# Standardize the features if needed (optional but often beneficial)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Create an XGBoost classifier\nmodel = XGBClassifier(objective='binary:logistic', random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n# Make predictions on the test set\ny_pred_prob = model.predict_proba(X_test)[:, 1]\n\n# Convert probability scores to binary predictions using a threshold\nthreshold = 0.5\ny_pred = (y_pred_prob >= threshold).astype(int)\n\n# Generate ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nroc_auc = auc(fpr, tpr)\n\n# Generate precision-recall curve\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\naverage_precision = average_precision_score(y_test, y_pred_prob)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:23:55.004684Z","iopub.execute_input":"2024-10-18T03:23:55.005088Z","iopub.status.idle":"2024-10-18T03:24:17.256288Z","shell.execute_reply.started":"2024-10-18T03:23:55.005049Z","shell.execute_reply":"2024-10-18T03:24:17.255225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n\n# Plot precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.step(recall, precision, color='b', where='post', label='Precision-Recall curve (AP = {:.2f})'.format(average_precision))\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc='upper right')\nplt.show()\n\nprint(f'Area under ROC curve (AUC-ROC): {roc_auc:.2f}')\nprint(f'Area under Precision-Recall curve (AUC-PR): {average_precision:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:24:17.258271Z","iopub.execute_input":"2024-10-18T03:24:17.258590Z","iopub.status.idle":"2024-10-18T03:24:17.799808Z","shell.execute_reply.started":"2024-10-18T03:24:17.258559Z","shell.execute_reply":"2024-10-18T03:24:17.799049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate confusion matrix and classification report\ncm = confusion_matrix(y_test, y_pred)\ncr = classification_report(y_test, y_pred)\n\nprint(\"Confusion Matrix:\")\nprint(cm)\n\nprint(\"\\nClassification Report:\")\nprint(cr)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:24:23.088596Z","iopub.execute_input":"2024-10-18T03:24:23.089303Z","iopub.status.idle":"2024-10-18T03:24:23.127633Z","shell.execute_reply.started":"2024-10-18T03:24:23.089268Z","shell.execute_reply":"2024-10-18T03:24:23.126848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns to exclude from features\nexclude_columns = ['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'los', 'los_type', \n                   'age', 'gender', 'insurance', 'race', 'PHYSICIAN REFERRAL']\n\n# Define your feature columns (exclude certain columns)\nfeature_columns = [col for col in train_data.columns if col not in exclude_columns]\n\n# Exclude non-numeric columns before scaling\nnumeric_columns = train_train_data[feature_columns].select_dtypes(include=['float64', 'int64']).columns\nX_train = train_train_data[numeric_columns]\ny_train = train_train_data['los_type']\nX_test = test_train_data[numeric_columns]\ny_test = test_train_data['los_type']\n\n# Convert 'Short' and 'Long' to numerical labels (0 and 1)\ny_train = y_train.map({'Short': 0, 'Long': 1})\ny_test = y_test.map({'Short': 0, 'Long': 1})\n\n# Standardize the features if needed (optional but often beneficial)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Extract insurance information for the test set\ninsurance_test = train_data.loc[y_test.index, 'insurance']\n\n# Create an XGBoost classifier\nmodel = XGBClassifier(objective='binary:logistic', random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Combine predictions and insurance information into a DataFrame\nresults_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Insurance': insurance_test})\n\n# Assess performance for each insurance type\ninsurance_types = results_df['Insurance'].unique()\nfor insurance_type in insurance_types:\n    subset = results_df[results_df['Insurance'] == insurance_type]\n    \n    # Generate confusion matrix and classification report\n    cm = confusion_matrix(subset['Actual'], subset['Predicted'])\n    cr = classification_report(subset['Actual'], subset['Predicted'])\n    \n    print(f\"Results for {insurance_type} insurance:\")\n    print(\"Confusion Matrix:\")\n    print(cm)\n    print(\"\\nClassification Report:\")\n    print(cr)\n    print('\\n' + '-'*50 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:25:04.638704Z","iopub.execute_input":"2024-10-18T03:25:04.639563Z","iopub.status.idle":"2024-10-18T03:25:14.272235Z","shell.execute_reply.started":"2024-10-18T03:25:04.639526Z","shell.execute_reply":"2024-10-18T03:25:14.271394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns to exclude from features\nexclude_columns = ['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'los', 'los_type', \n                   'age', 'gender', 'insurance', 'race', 'PHYSICIAN REFERRAL']\n\n# Define your feature columns (exclude certain columns)\nfeature_columns = [col for col in train_data.columns if col not in exclude_columns]\n\n# Exclude non-numeric columns before scaling\nnumeric_columns = train_train_data[feature_columns].select_dtypes(include=['float64', 'int64']).columns\nX_train = train_train_data[numeric_columns]\ny_train = train_train_data['los_type']\nX_test = test_train_data[numeric_columns]\ny_test = test_train_data['los_type']\n\n# Convert 'Short' and 'Long' to numerical labels (0 and 1)\ny_train = y_train.map({'Short': 0, 'Long': 1})\ny_test = y_test.map({'Short': 0, 'Long': 1})\n\n# Standardize the features if needed (optional but often beneficial)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Extract insurance information for the test set\nrace_test = train_data.loc[y_test.index, 'race']\n\n# Create an XGBoost classifier\nmodel = XGBClassifier(objective='binary:logistic', random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Combine predictions and insurance information into a DataFrame\nresults_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Race': race_test})\n\n# Get unique race types\nrace_types = results_df['Race'].unique()\n# Assess performance for each race type\nfor race_type in race_types:\n    subset = results_df[results_df['Race'] == race_type]\n    \n    # Generate confusion matrix and classification report\n    cm = confusion_matrix(subset['Actual'], subset['Predicted'])\n    cr = classification_report(subset['Actual'], subset['Predicted'])\n    \n    print(f\"Results for {race_type} race:\")\n    print(\"Confusion Matrix:\")\n    print(cm)\n    print(\"\\nClassification Report:\")\n    print(cr)\n    print('\\n' + '-'*50 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:25:26.312029Z","iopub.execute_input":"2024-10-18T03:25:26.312941Z","iopub.status.idle":"2024-10-18T03:25:36.195968Z","shell.execute_reply.started":"2024-10-18T03:25:26.312903Z","shell.execute_reply":"2024-10-18T03:25:36.195219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def equalized_odds(y_true, y_pred, sensitive_feature):\n    # Calculate confusion matrices\n    cm_protected = confusion_matrix(y_true[sensitive_feature], y_pred[sensitive_feature])\n    cm_non_protected = confusion_matrix(y_true[~sensitive_feature], y_pred[~sensitive_feature])\n\n    # Calculate True Positive Rate (sensitivity) and False Positive Rate (fallout) for both groups\n    tpr_protected = cm_protected[1, 1] / cm_protected.sum(axis=1)[1]\n    fpr_protected = cm_protected[0, 1] / cm_protected.sum(axis=1)[0]\n\n    tpr_non_protected = cm_non_protected[1, 1] / cm_non_protected.sum(axis=1)[1]\n    fpr_non_protected = cm_non_protected[0, 1] / cm_non_protected.sum(axis=1)[0]\n\n    # Calculate the absolute difference in TPR and FPR between protected and non-protected groups\n    tpr_difference = abs(tpr_protected - tpr_non_protected)\n    fpr_difference = abs(fpr_protected - fpr_non_protected)\n\n    return tpr_difference + fpr_difference\n\ndef demographic_parity(y_pred, sensitive_feature):\n    proportion_protected = y_pred[sensitive_feature].mean()\n    proportion_non_protected = y_pred[~sensitive_feature].mean()\n\n    return abs(proportion_protected - proportion_non_protected)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:25:36.197239Z","iopub.execute_input":"2024-10-18T03:25:36.197691Z","iopub.status.idle":"2024-10-18T03:25:36.203542Z","shell.execute_reply.started":"2024-10-18T03:25:36.197660Z","shell.execute_reply":"2024-10-18T03:25:36.202800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns to exclude from features\nexclude_columns = ['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'los', 'los_type', \n                   'age', 'gender', 'insurance', 'race', 'PHYSICIAN REFERRAL']\n\n# Define your feature columns (exclude certain columns)\nfeature_columns = [col for col in validation_data.columns if col not in exclude_columns]\n\n# Exclude non-numeric columns before scaling\nnumeric_columns = validation_data[feature_columns].select_dtypes(include=['float64', 'int64']).columns\nX_verify = validation_data[numeric_columns]\ny_verify = validation_data['los_type']\n\n# Convert 'Short' and 'Long' to numerical labels (0 and 1)\ny_verify = y_verify.map({'Short': 0, 'Long': 1})\n\n# Standardize the features using the same scaler from the training set\nX_verify = scaler.transform(X_verify)\n\n# Make predictions on the verification set using the existing model\ny_verify_pred_prob = model.predict_proba(X_verify)[:, 1]\n\n# Convert probability scores to binary predictions using a threshold\ny_verify_pred = (y_verify_pred_prob > threshold).astype(int)\n\n# Generate ROC curve for the verification set\nfpr_verify, tpr_verify, thresholds_verify = roc_curve(y_verify, y_verify_pred_prob)\nroc_auc_verify = auc(fpr_verify, tpr_verify)\n\n# Generate precision-recall curve for the verification set\nprecision_verify, recall_verify, _ = precision_recall_curve(y_verify, y_verify_pred_prob)\naverage_precision_verify = average_precision_score(y_verify, y_verify_pred_prob)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:25:51.046735Z","iopub.execute_input":"2024-10-18T03:25:51.047090Z","iopub.status.idle":"2024-10-18T03:25:51.873520Z","shell.execute_reply.started":"2024-10-18T03:25:51.047062Z","shell.execute_reply":"2024-10-18T03:25:51.872631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot ROC curve for the verification set\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_verify, tpr_verify, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc_verify))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve - Validation Set')\nplt.legend(loc='lower right')\nplt.show()\n\n# Plot precision-recall curve for the verification set\nplt.figure(figsize=(8, 6))\nplt.step(recall_verify, precision_verify, color='b', where='post', label='Precision-Recall curve (AP = {:.2f})'.format(average_precision_verify))\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve - Validation Set')\nplt.legend(loc='upper right')\nplt.show()\n\nprint(f'Area under ROC curve (AUC-ROC) for Verification Set: {roc_auc_verify:.2f}')\nprint(f'Area under Precision-Recall curve (AUC-PR) for Verification Set: {average_precision_verify:.2f}')\n\n# Generate confusion matrix and classification report for the verification set\ncm_verify = confusion_matrix(y_verify, y_verify_pred)\ncr_verify = classification_report(y_verify, y_verify_pred)\n\nprint(\"Confusion Matrix for Validation Set:\")\nprint(cm_verify)\n\nprint(\"\\nClassification Report for Validation Set:\")\nprint(cr_verify)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:25:59.886861Z","iopub.execute_input":"2024-10-18T03:25:59.887733Z","iopub.status.idle":"2024-10-18T03:26:00.284892Z","shell.execute_reply.started":"2024-10-18T03:25:59.887693Z","shell.execute_reply":"2024-10-18T03:26:00.284098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have the true labels for the validation set\ny_true = validation_data['los_type'].map({'Short': 0, 'Long': 1})\n\n# Assuming 'validation_data' has predictions stored in 'y_verify_pred'\ncm_verify = confusion_matrix(y_true, y_verify_pred)\n\n# Extract values from the confusion matrix\ntn, fp, fn, tp = cm_verify.ravel()\n\n# Calculate equalized odds\nequalized_odds_race = (tp / (tp + fn)) / (fp / (fp + tn))\n\nprint(equalized_odds_race)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:26:07.797483Z","iopub.execute_input":"2024-10-18T03:26:07.798269Z","iopub.status.idle":"2024-10-18T03:26:07.806999Z","shell.execute_reply.started":"2024-10-18T03:26:07.798234Z","shell.execute_reply":"2024-10-18T03:26:07.806341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns to exclude from features\nexclude_columns = ['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'los', 'los_type', \n                   'age', 'gender', 'race', 'PHYSICIAN REFERRAL']\n\n# Define your feature columns (exclude certain columns)\nfeature_columns = [col for col in validation_data.columns if col not in exclude_columns]\n\n# Exclude non-numeric columns before scaling\nnumeric_columns = validation_data[feature_columns].select_dtypes(include=['float64', 'int64']).columns\nX = validation_data[numeric_columns]\ny = validation_data['los_type']\n\n# Convert 'Short' and 'Long' to numerical labels (0 and 1)\ny = y.map({'Short': 0, 'Long': 1})\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Extract insurance information for the test set\ninsurance_test = validation_data.loc[y_test.index, 'insurance']\n\n# Standardize the features if needed (optional but often beneficial)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Create an XGBoost classifier\nmodel = XGBClassifier(objective='binary:logistic', random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Combine predictions and insurance information into a DataFrame\nresults_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Insurance': insurance_test})\n\n# Assess performance for each insurance type\ninsurance_types = results_df['Insurance'].unique()\nfor insurance_type in insurance_types:\n    subset = results_df[results_df['Insurance'] == insurance_type]\n    \n    # Generate confusion matrix and classification report\n    cm = confusion_matrix(subset['Actual'], subset['Predicted'])\n    cr = classification_report(subset['Actual'], subset['Predicted'])\n    \n    print(f\"Results for {insurance_type} insurance:\")\n    print(\"Confusion Matrix:\")\n    print(cm)\n    print(\"\\nClassification Report:\")\n    print(cr)\n    print('\\n' + '-'*50 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:26:23.037289Z","iopub.execute_input":"2024-10-18T03:26:23.037662Z","iopub.status.idle":"2024-10-18T03:26:30.005114Z","shell.execute_reply.started":"2024-10-18T03:26:23.037630Z","shell.execute_reply":"2024-10-18T03:26:30.004128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns to exclude from features\nexclude_columns = ['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'los', 'los_type', \n                   'age', 'gender', 'race', 'PHYSICIAN REFERRAL']\n\n# Define your feature columns (exclude certain columns)\nfeature_columns = [col for col in validation_data.columns if col not in exclude_columns]\n\n# Exclude non-numeric columns before scaling\nnumeric_columns = validation_data[feature_columns].select_dtypes(include=['float64', 'int64']).columns\nX = validation_data[numeric_columns]\ny = validation_data['los_type']\n\n# Convert 'Short' and 'Long' to numerical labels (0 and 1)\ny = y.map({'Short': 0, 'Long': 1})\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Extract insurance information for the test set\ninsurance_test = validation_data.loc[y_test.index, 'insurance']\n\n# Standardize the features if needed (optional but often beneficial)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Create an XGBoost classifier\nmodel = XGBClassifier(objective='binary:logistic', random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Combine predictions and insurance information into a DataFrame\nresults_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Insurance': insurance_test})\n\n# Assess performance for each insurance type\ninsurance_types = results_df['Insurance'].unique()\n\nfor insurance_type in insurance_types:\n    subset = results_df[results_df['Insurance'] == insurance_type]\n    \n    # Generate confusion matrix and classification report\n    cm = confusion_matrix(subset['Actual'], subset['Predicted'])\n    cr = classification_report(subset['Actual'], subset['Predicted'])\n    \n    print(f\"Results for {insurance_type} insurance:\")\n    print(\"Confusion Matrix:\")\n    print(cm)\n    print(\"\\nClassification Report:\")\n    print(cr)\n\n    # Calculate Type 1 and Type 2 errors\n    type1_error = cm[0, 1]  # False Positives\n    type2_error = cm[1, 0]  # False Negatives\n\n    print(f\"Type 1 Error (False Positives): {type1_error}\")\n    print(f\"Type 2 Error (False Negatives): {type2_error}\")\n\n    print('\\n' + '-'*50 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:26:40.630893Z","iopub.execute_input":"2024-10-18T03:26:40.631670Z","iopub.status.idle":"2024-10-18T03:26:47.167549Z","shell.execute_reply.started":"2024-10-18T03:26:40.631637Z","shell.execute_reply":"2024-10-18T03:26:47.166646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns to exclude from features\nexclude_columns = ['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'los', 'los_type', \n                   'age', 'gender', 'race', 'PHYSICIAN REFERRAL']\n\n# Define your feature columns (exclude certain columns)\nfeature_columns = [col for col in validation_data.columns if col not in exclude_columns]\n\n# Exclude non-numeric columns before scaling\nnumeric_columns = validation_data[feature_columns].select_dtypes(include=['float64', 'int64']).columns\nX = validation_data[numeric_columns]\ny = validation_data['los_type']\n\n# Convert 'Short' and 'Long' to numerical labels (0 and 1)\ny = y.map({'Short': 0, 'Long': 1})\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Extract insurance information for the test set\nrace_test = validation_data.loc[y_test.index, 'race']\n\n# Standardize the features if needed (optional but often beneficial)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Create an XGBoost classifier\nmodel = XGBClassifier(objective='binary:logistic', random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Combine predictions and insurance information into a DataFrame\nresults_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Race': race_test})\n\n# Assess performance for each race type\nfor race_type in race_types:\n    subset = results_df[results_df['Race'] == race_type]\n    \n    # Generate confusion matrix and classification report\n    cm = confusion_matrix(subset['Actual'], subset['Predicted'])\n    cr = classification_report(subset['Actual'], subset['Predicted'])\n    \n    print(f\"Results for {race_type} race:\")\n    print(\"Confusion Matrix:\")\n    print(cm)\n    print(\"\\nClassification Report:\")\n    print(cr)\n    print('\\n' + '-'*50 + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T03:26:57.162424Z","iopub.execute_input":"2024-10-18T03:26:57.162763Z","iopub.status.idle":"2024-10-18T03:27:02.228143Z","shell.execute_reply.started":"2024-10-18T03:26:57.162736Z","shell.execute_reply":"2024-10-18T03:27:02.227101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}